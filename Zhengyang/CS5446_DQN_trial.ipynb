{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NkxxgzcMmy0R",
    "outputId": "48d10d26-7514-4303-e1fe-9c3e2161b4f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  5 00:21:39 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 511.79       Driver Version: 511.79       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   42C    P8    12W / 185W |    653MiB /  8192MiB |      8%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2880    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A      3088    C+G   ...e\\Current\\LogiOverlay.exe    N/A      |\n",
      "|    0   N/A  N/A      5308    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11504    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     13152    C+G   ...8\\extracted\\WeChatApp.exe    N/A      |\n",
      "|    0   N/A  N/A     16684    C+G   ... Host\\Razer Synapse 3.exe    N/A      |\n",
      "|    0   N/A  N/A     17440    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     20520    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     20588    C+G   ...ontend\\Docker Desktop.exe    N/A      |\n",
      "|    0   N/A  N/A     21456    C+G   ...erFast 9\\SilverFast 9.exe    N/A      |\n",
      "|    0   N/A  N/A     23220    C+G                                   N/A      |\n",
      "|    0   N/A  N/A     23508    C+G   ...Roaming\\Zoom\\bin\\Zoom.exe    N/A      |\n",
      "|    0   N/A  N/A     25952    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     27572    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     29044    C+G   ...obeNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     29328    C+G   ...arp.BrowserSubprocess.exe    N/A      |\n",
      "|    0   N/A  N/A     29412    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     29544    C+G   ...app-2.6.10\\SourceTree.exe    N/A      |\n",
      "|    0   N/A  N/A     30088    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     31760    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     31824    C+G                                   N/A      |\n",
      "|    0   N/A  N/A     32920    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     33364    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     34752    C+G   ...erver\\YourPhoneServer.exe    N/A      |\n",
      "|    0   N/A  N/A     36504    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     39760    C+G   ...tracted\\WechatBrowser.exe    N/A      |\n",
      "|    0   N/A  N/A     42976    C+G   ...ekyb3d8bbwe\\onenoteim.exe    N/A      |\n",
      "|    0   N/A  N/A     51040    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WMeV606OgsDH",
    "outputId": "f7c6d29b-c4c2-4ab8-ef82-32e2140dcbdc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.10.5 in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (0.10.5)\n",
      "Requirement already satisfied: numpy>=1.10.4 in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from gym==0.10.5) (1.21.1)\n",
      "Requirement already satisfied: requests>=2.0 in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from gym==0.10.5) (2.26.0)\n",
      "Requirement already satisfied: six in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from gym==0.10.5) (1.16.0)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from gym==0.10.5) (1.5.16)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from requests>=2.0->gym==0.10.5) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from requests>=2.0->gym==0.10.5) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from requests>=2.0->gym==0.10.5) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from requests>=2.0->gym==0.10.5) (2021.5.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym==0.10.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///G:/Git/NUS_MComp/CS5446/cs5446_grp_proj/Gym-Snake-master\n",
      "Requirement already satisfied: gym in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from gym-snake==0.0.1) (0.10.5)\n",
      "Requirement already satisfied: numpy in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from gym-snake==0.0.1) (1.21.1)\n",
      "Requirement already satisfied: matplotlib in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from gym-snake==0.0.1) (3.4.2)\n",
      "Requirement already satisfied: requests>=2.0 in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from gym->gym-snake==0.0.1) (2.26.0)\n",
      "Requirement already satisfied: six in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from gym->gym-snake==0.0.1) (1.16.0)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from gym->gym-snake==0.0.1) (1.5.16)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from requests>=2.0->gym->gym-snake==0.0.1) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from requests>=2.0->gym->gym-snake==0.0.1) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from requests>=2.0->gym->gym-snake==0.0.1) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from requests>=2.0->gym->gym-snake==0.0.1) (1.26.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from matplotlib->gym-snake==0.0.1) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from matplotlib->gym-snake==0.0.1) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from matplotlib->gym-snake==0.0.1) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from matplotlib->gym-snake==0.0.1) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in g:\\tool\\miniconda\\envs\\deeplearn_course\\lib\\site-packages (from matplotlib->gym-snake==0.0.1) (8.3.1)\n",
      "Installing collected packages: gym-snake\n",
      "  Attempting uninstall: gym-snake\n",
      "    Found existing installation: gym-snake 0.0.1\n",
      "    Uninstalling gym-snake-0.0.1:\n",
      "      Successfully uninstalled gym-snake-0.0.1\n",
      "  Running setup.py develop for gym-snake\n",
      "Successfully installed gym-snake-0.0.1\n"
     ]
    }
   ],
   "source": [
    "# Install gym-snake\n",
    "!pip install -e ./Gym-Snake-master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "e1xhY1yvZWAB"
   },
   "outputs": [],
   "source": [
    "import random, time, gym, sys\n",
    "import gym.spaces\n",
    "from gym.wrappers.monitor import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from collections import namedtuple\n",
    "import pickle\n",
    "import time\n",
    "import uuid\n",
    "import os\n",
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "OptimizerSpec = namedtuple(\"OptimizerSpec\", [\"constructor\", \"kwargs\", \"lr_schedule\"])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_snake\n",
    "# env=gym.make('snake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5Jv6WGqkAfC"
   },
   "source": [
    "## Set Up Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNDYo5uqUxJr",
    "outputId": "1a236355-1130-4310-e51c-f334375b30ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed = 3\n",
      "<Monitor<SnakeEnv<snake-v0>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Tool\\Miniconda\\envs\\deeplearn_course\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# Set snake game environment and set up\n",
    "env='snake-v0'\n",
    "seed=3\n",
    "num_steps=5e4\n",
    "double_q=True\n",
    "\n",
    "\n",
    "if seed is None:\n",
    "    seed = random.randint(0, 9999)\n",
    "print('random seed = {}'.format(seed))\n",
    "exp_name = 'double-dqn'\n",
    "\n",
    "logdir = exp_name+ '_' +env+ '_' +time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "logdir = os.path.join('data_dqn', logdir)\n",
    "\n",
    "def set_global_seeds(i):\n",
    "    np.random.seed(i)\n",
    "    random.seed(i)\n",
    "\n",
    "env = gym.make(env)\n",
    "set_global_seeds(seed)\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "expt_dir = os.path.join(logdir, \"gym\")\n",
    "env = Monitor(env, expt_dir, force=True, video_callable=False)\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# last_obs = env.reset()\n",
    "# %matplotlib notebook\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(last_obs.shape)\n",
    "# print(last_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %matplotlib notebook\n",
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "js2DALaqtp2f"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.pool1  = nn.MaxPool2d(10,10)\n",
    "        self.dense1 = nn.Linear(input_size, 1024)\n",
    "        self.dense2 = nn.Linear(1024, 1024)\n",
    "        self.dense3 = nn.Linear(1024, num_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(x)\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = F.tanh(self.dense1(x))\n",
    "        x = F.tanh(self.dense2(x))\n",
    "        out = self.dense3(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIyQdLRFkGxj"
   },
   "source": [
    "## Set Up Exploration Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OXoWTuTySRwt"
   },
   "outputs": [],
   "source": [
    "def linear_interpolation(l, r, alpha):\n",
    "    return l + alpha * (r - l)\n",
    "\n",
    "class PiecewiseSchedule(object):\n",
    "\n",
    "    def __init__(self, endpoints, interpolation=linear_interpolation, outside_value=None):\n",
    "        \"\"\"Piecewise schedule.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        endpoints: [(int, int)]\n",
    "            list of pairs `(time, value)` meanining that schedule should output\n",
    "            `value` when `t==time`. All the values for time must be sorted in\n",
    "            an increasing order. When t is between two times, e.g. `(time_a, value_a)`\n",
    "            and `(time_b, value_b)`, such that `time_a <= t < time_b` then value outputs\n",
    "            `interpolation(value_a, value_b, alpha)` where alpha is a fraction of\n",
    "            time passed between `time_a` and `time_b` for time `t`.\n",
    "        interpolation: lambda float, float, float: float\n",
    "            a function that takes value to the left and to the right of t according\n",
    "            to the `endpoints`. Alpha is the fraction of distance from left endpoint to\n",
    "            right endpoint that t has covered. See linear_interpolation for example.\n",
    "        outside_value: float\n",
    "            if the value is requested outside of all the intervals sepecified in\n",
    "            `endpoints` this value is returned. If None then AssertionError is\n",
    "            raised when outside value is requested.\n",
    "        \"\"\"\n",
    "        idxes = [e[0] for e in endpoints]\n",
    "        assert idxes == sorted(idxes)\n",
    "        self._interpolation = interpolation\n",
    "        self._outside_value = outside_value\n",
    "        self._endpoints      = endpoints\n",
    "\n",
    "    def value(self, t):\n",
    "        \"\"\"See Schedule.value\"\"\"\n",
    "        for (l_t, l), (r_t, r) in zip(self._endpoints[:-1], self._endpoints[1:]):\n",
    "            if l_t <= t and t < r_t:\n",
    "                alpha = float(t - l_t) / (r_t - l_t)\n",
    "                return self._interpolation(l, r, alpha)\n",
    "\n",
    "        # t does not belong to any of the pieces, so doom.\n",
    "        assert self._outside_value is not None\n",
    "        return self._outside_value\n",
    "\n",
    "exploration_schedule = PiecewiseSchedule([\n",
    "        (0,   1.00),\n",
    "        (5e3, 0.03),\n",
    "        (1e4, 0.01),\n",
    "    ], outside_value=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3MvN_jkqpcy",
    "outputId": "e674ce83-76db-4e6b-cddc-3c2ce8063253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0 5000.0 0.03\n",
      "5000.0 0.03 10000.0 0.01\n"
     ]
    }
   ],
   "source": [
    "_endpoints = [\n",
    "        (0,   1.00),\n",
    "        (5e3, 0.03),\n",
    "        (1e4, 0.01),\n",
    "    ]\n",
    "for (l_t, l), (r_t, r) in zip(_endpoints[:-1], _endpoints[1:]):\n",
    "    print(l_t, l, r_t, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbUcHigekaAh"
   },
   "source": [
    "## Set Up Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "14k1aDmnSpku"
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# REPLAY BUFFER\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class BasicBuffer:\n",
    "\n",
    "  def __init__(self, max_size):\n",
    "      self.max_size = max_size\n",
    "      self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "  def push(self, state, action, reward, next_state, done):\n",
    "      experience = (state, action, np.array([reward]), next_state, done)\n",
    "      self.buffer.append(experience)\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "      state_batch = []\n",
    "      action_batch = []\n",
    "      reward_batch = []\n",
    "      next_state_batch = []\n",
    "      done_batch = []\n",
    "\n",
    "      batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "      for experience in batch:\n",
    "          state, action, reward, next_state, done = experience\n",
    "          state_batch.append(state)\n",
    "          action_batch.append(action)\n",
    "          reward_batch.append(reward)\n",
    "          next_state_batch.append(next_state)\n",
    "          done_batch.append(done)\n",
    "\n",
    "      return (state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.buffer)\n",
    "\n",
    "def get_wrapper_by_name(env, classname):\n",
    "    currentenv = env\n",
    "    while True:\n",
    "        if classname in currentenv.__class__.__name__:\n",
    "            return currentenv\n",
    "        elif isinstance(env, gym.Wrapper):\n",
    "            currentenv = currentenv.env\n",
    "        else:\n",
    "            raise ValueError(\"Couldn't find wrapper named %s\"%classname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kvP8UR7kskG"
   },
   "source": [
    "## Run Q-Learning Algorithm\n",
    "\n",
    "## Noted you need to implement *Double DQN* in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hqNzlrOIR4dh"
   },
   "outputs": [],
   "source": [
    "class QLearner(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 q_func,\n",
    "                 t_func,\n",
    "                 optimizer_spec,\n",
    "                 exploration,\n",
    "                 replay_buffer_size,\n",
    "                 batch_size,\n",
    "                 gamma,\n",
    "                 learning_starts,\n",
    "                 target_update_freq,\n",
    "                 grad_norm_clipping,\n",
    "                 double_q=True,\n",
    "                 max_steps=2e8,\n",
    "                 cartpole=True):\n",
    "        \"\"\"Run Deep Q-learning algorithm.\n",
    "\n",
    "        You can specify your own convnet using `q_func`.\n",
    "        All schedules are w.r.t. total number of steps taken in the environment.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env: gym.Env\n",
    "            gym environment to train on.\n",
    "        q_func: function\n",
    "            Model to use for computing the q function. \n",
    "        t_func: function\n",
    "            Model to use for computing the target q function. \n",
    "        optimizer_spec: OptimizerSpec\n",
    "            Specifying the constructor and kwargs, as well as learning rate schedule\n",
    "            for the optimizer\n",
    "        exploration: \n",
    "            exploration schedule\n",
    "        replay_buffer_size: int\n",
    "            How many memories to store in the replay buffer.\n",
    "        batch_size: int\n",
    "            How many transitions to sample each time experience is replayed.\n",
    "        gamma: float\n",
    "            Discount Factor\n",
    "        learning_starts: int\n",
    "            After how many environment steps to start replaying experiences\n",
    "        target_update_freq: int\n",
    "            How many experience replay rounds (not steps!) to perform between\n",
    "            each update to the target Q network\n",
    "        grad_norm_clipping: float or None\n",
    "            If not None gradients' norms are clipped to this value.\n",
    "        double_q: bool\n",
    "            If True, use double Q-learning to compute target values. Otherwise, vanilla DQN.\n",
    "            https://papers.nips.cc/paper/3964-double-q-learning.pdf\n",
    "        max_steps: int\n",
    "            Maximum number of training steps. The number of *frames* is 4x this\n",
    "            quantity (modulo the initial random no-op steps).\n",
    "        cartpole: bool\n",
    "            If True, CartPole-v0.\n",
    "        \"\"\"\n",
    "#         print(env.observation_space)\n",
    "#         print(env.action_space)\n",
    "#         assert type(env.observation_space) == gym.spaces.Box\n",
    "#         assert type(env.action_space)      == gym.spaces.Discrete\n",
    "        self.q_func = q_func\n",
    "        self.t_func = t_func\n",
    "        self.max_steps = int(max_steps)\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.optimizer = optimizer_spec\n",
    "        self.exploration = exploration\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.learning_starts = learning_starts\n",
    "        self.double_q = double_q\n",
    "        self.cartpole = cartpole\n",
    "        self.env = env\n",
    "        self.replay_buffer = BasicBuffer(max_size=replay_buffer_size)\n",
    "\n",
    "        # self.input_shape = self.env.observation_space.shape # should be (4,)\n",
    "        self.num_actions = self.env.action_space.n\n",
    "\n",
    "        self.num_param_updates = 0\n",
    "        self.mean_episode_reward      = -float('nan')\n",
    "        self.std_episode_reward       = -float('nan')\n",
    "        self.best_mean_episode_reward = -float('inf') \n",
    "        self.steps = []\n",
    "        self.mean_rewards = []\n",
    "        self.log_every_n_steps = 200\n",
    "        self.start_time = time.time()\n",
    "        self.last_obs = self.env.reset()\n",
    "        self.t = 0\n",
    "\n",
    "\n",
    "    def step_env(self):\n",
    "        \"\"\"Step the env and store the transition.\n",
    "\n",
    "        At this point, `self.last_obs` contains the latest observation that was\n",
    "        recorded from the simulator. Here, your code\n",
    "        needs to store this observation and outcome (reward, next observation,\n",
    "        etc.) into the replay buffer while doing one step in the env simulator.\n",
    "\n",
    "        At the end of this block of code, the simulator should have been\n",
    "        advanced one step, the replay buffer should contain one more transition,\n",
    "        and, `self.last_obs` must point to the new latest observation.\n",
    "\n",
    "        Useful functions you'll need to call:\n",
    "\n",
    "            self.env.step(action)\n",
    "            self.replay_buffer.push(self.last_obs, action, reward, next_obs, done)\n",
    "\n",
    "        This steps the environment forward one step. And:\n",
    "\n",
    "            obs = self.env.reset()\n",
    "\n",
    "        This resets the environment if you reached an episode boundary.  Call\n",
    "        `self.env.reset()` to get a new observation if `done=True`. For CartPole, \n",
    "        this is guaranteed to start a new episode as they don't have a\n",
    "        notion of 'ale.lives' in them.\n",
    "\n",
    "        Don't forget to include epsilon greedy exploration!  \n",
    "        \"\"\"\n",
    "\n",
    "        # do below steps to collect dataset until replay_buffer_size is reached\n",
    "\n",
    "        # Exploration\n",
    "        if random.random() < self.exploration.value(self.t):\n",
    "            # perform random action 0:UP 1:RIGHT 2:DOWN 3:LEFT\n",
    "            action = random.randint(0, 3)\n",
    "        else:\n",
    "            # perform action that maximizes Q\n",
    "            # compute Q for all actions\n",
    "            input_to_be_formatted = torch.Tensor(self.last_obs)\n",
    "            #print(input_to_be_formatted.size())\n",
    "            input = torch.zeros(input_to_be_formatted.size(2), input_to_be_formatted.size(0), input_to_be_formatted.size(1))\n",
    "            #print(input.size())\n",
    "            input[0, :, :] = input_to_be_formatted[:, :, 0]\n",
    "            input[1, :, :] = input_to_be_formatted[:, :, 1]\n",
    "            input[2, :, :] = input_to_be_formatted[:, :, 2]\n",
    "            input = input.to(device)\n",
    "            scores = self.q_func(input)\n",
    "            action = torch.argmax(scores).item()\n",
    "\n",
    "        # step the environment with action\n",
    "        next_obs, reward, done, _ = self.env.step(action)\n",
    "\n",
    "        # record to replay buffer\n",
    "        self.replay_buffer.push(self.last_obs, action, reward, next_obs, done)\n",
    "\n",
    "        if done == True:\n",
    "            print(\"done\")\n",
    "            self.last_obs = self.env.reset()\n",
    "        else:\n",
    "            self.last_obs = next_obs\n",
    "\n",
    "\n",
    "    def update_model(self):\n",
    "        \"\"\"Perform experience replay and train the network.\n",
    "\n",
    "        This is only done if the replay buffer contains enough samples for us to\n",
    "        learn something useful -- until then, the model will not be initialized\n",
    "        and random actions should be taken.  Training consists of four steps:\n",
    "        \n",
    "        3.a: Use the replay buffer to sample a batch of transitions. See the\n",
    "        replay buffer code for function definitions. You may use this function:\n",
    "\n",
    "          self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        3.b: Compute the total Bellman error in a batch. You can use MSE loss to\n",
    "        achieve it.\n",
    "        \n",
    "        3.c: Perform a gradient step and update the network parameters to reduce\n",
    "        total_error. \n",
    "        \n",
    "        3.d: Periodically update the target network (self.t_func) every \n",
    "        `self.target_update_freq` steps.\n",
    "\n",
    "        \"\"\"\n",
    "        if (self.t > self.learning_starts and \\\n",
    "            len(self.replay_buffer) > self.batch_size):\n",
    " \n",
    "            # sample batch of transitions from replay buffer\n",
    "            state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "            # q function scores and yt_DoubleDQN\n",
    "            q_func_scores = torch.zeros(self.batch_size)\n",
    "            q_func_values = torch.zeros(self.batch_size)\n",
    "            yt_ddqn = torch.zeros(self.batch_size)\n",
    "\n",
    "            reward_batch = torch.Tensor(reward_batch)\n",
    "            next_state_batch = torch.Tensor(next_state_batch)\n",
    "            state_batch = torch.Tensor(state_batch)\n",
    "            reward_batch = reward_batch.to(device)\n",
    "            \n",
    "            next_state_batch_formatted = torch.zeros(next_state_batch.size(0), next_state_batch.size(3), next_state_batch.size(1), next_state_batch.size(2))\n",
    "            next_state_batch_formatted[:, 0, :, :] = next_state_batch[:, :, :, 0]\n",
    "            next_state_batch_formatted[:, 1, :, :] = next_state_batch[:, :, :, 1]\n",
    "            next_state_batch_formatted[:, 2, :, :] = next_state_batch[:, :, :, 2]\n",
    "            next_state_batch_formatted = next_state_batch_formatted.to(device)\n",
    "            next_state_batch_formatted.requires_grad_()\n",
    "            \n",
    "            state_batch_formatted = torch.zeros(state_batch.size(0), state_batch.size(3), state_batch.size(1), state_batch.size(2))\n",
    "            state_batch_formatted[:, 0, :, :] = state_batch[:, :, :, 0]\n",
    "            state_batch_formatted[:, 1, :, :] = state_batch[:, :, :, 1]\n",
    "            state_batch_formatted[:, 2, :, :] = state_batch[:, :, :, 2]\n",
    "            state_batch_formatted = state_batch_formatted.to(device)\n",
    "            state_batch_formatted.requires_grad_()\n",
    "\n",
    "            # print(next_state_batch.size())\n",
    "            #next_state_batch_formatted = \n",
    "            best_action_next_state = torch.argmax(self.q_func(next_state_batch_formatted), dim=1)\n",
    "            # print(best_action_next_state)\n",
    "            target_scores_next_state = self.t_func(next_state_batch_formatted)\n",
    "            # print(target_scores_next_state)\n",
    "            q_func_scores = self.q_func(state_batch_formatted)\n",
    "            for i in range(self.batch_size):\n",
    "                yt_ddqn[i] = reward_batch[i] + self.gamma * target_scores_next_state[i][best_action_next_state[i].item()] * (1 - done_batch[i])\n",
    "                q_func_values[i] = q_func_scores[i][action_batch[i]]\n",
    "\n",
    "            # compute loss\n",
    "            # print(q_func_values)\n",
    "            # print(yt_ddqn)\n",
    "            criterion = nn.MSELoss()\n",
    "            loss = criterion(q_func_values, yt_ddqn)\n",
    "            # print(q_func_values)\n",
    "            # print(yt_ddqn)\n",
    "            # print(loss)\n",
    "\n",
    "            # back propagate and optimizer.step\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.q_func.parameters(), 10)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # if self.target_update_freq is reached, update target function\n",
    "            if self.t % self.target_update_freq == 0:\n",
    "                # update target function parameters\n",
    "                self.t_func.load_state_dict(self.q_func.state_dict())\n",
    "\n",
    "            self.num_param_updates += 1\n",
    "        self.t += 1\n",
    "\n",
    "    def log_progress(self):\n",
    "        episode_rewards = get_wrapper_by_name(self.env, \"Monitor\").get_episode_rewards()\n",
    "\n",
    "        if len(episode_rewards) > 0:\n",
    "            self.mean_episode_reward = np.mean(episode_rewards[-10:])\n",
    "            self.std_episode_reward  = np.std(episode_rewards[-10:])\n",
    "        if len(episode_rewards) > 10:\n",
    "            self.best_mean_episode_reward = \\\n",
    "                max(self.best_mean_episode_reward, self.mean_episode_reward)\n",
    "\n",
    "        if self.t % self.log_every_n_steps == 0:\n",
    "            self.steps.append(self.t)\n",
    "            self.mean_rewards.append(self.mean_episode_reward)\n",
    "            hours = (time.time() - self.start_time) / (60.*60.)\n",
    "            print(\"Steps: \",                 self.t)\n",
    "            print(\"Avg_Last_10_Episodes\", self.mean_episode_reward)\n",
    "            print(\"Std_Last_10_Episodes\", self.std_episode_reward)\n",
    "            print(\"Best_Avg_10_Episodes\", self.best_mean_episode_reward)\n",
    "            print(\"Num_Episodes\",          len(episode_rewards))\n",
    "            print(\"Exploration_Epsilon\",   self.exploration.value(self.t))\n",
    "            print(\"Elapsed_Time_Hours\",    hours)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "t6xQNm0d6mVu"
   },
   "outputs": [],
   "source": [
    "def dqn_learn(*args, **kwargs):\n",
    "    alg = QLearner(*args, **kwargs)\n",
    "    while True:\n",
    "        # \n",
    "        #print(\"play the game\")\n",
    "        alg.step_env()\n",
    "        #print(\"update model\")\n",
    "        alg.update_model()\n",
    "        #print(\"log progress\")\n",
    "        alg.log_progress()\n",
    "        \n",
    "        if alg.t % 100 == 0:\n",
    "            print(\"\\nt = {} out of max_steps = {}\".format(alg.t, alg.max_steps))\n",
    "        if alg.t > alg.max_steps:\n",
    "            print(\"\\nt = {} exceeds max_steps = {}\".format(alg.t, alg.max_steps))\n",
    "            break\n",
    "    return alg.steps, alg.mean_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmaQDyOotkHF",
    "outputId": "fbdebb91-3cf9-4c3d-f8f6-9d00083a2778",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 100 out of max_steps = 50000\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Tool\\Miniconda\\envs\\deeplearn_course\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "G:\\Tool\\Miniconda\\envs\\deeplearn_course\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "Steps:  200\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -inf\n",
      "Num_Episodes 7\n",
      "Exploration_Epsilon 0.9612\n",
      "Elapsed_Time_Hours 0.00012249979707929824\n",
      "\n",
      "t = 200 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 300 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  400\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -1.0\n",
      "Num_Episodes 18\n",
      "Exploration_Epsilon 0.9224\n",
      "Elapsed_Time_Hours 0.00013361089759402804\n",
      "\n",
      "t = 400 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 500 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  600\n",
      "Avg_Last_10_Episodes -0.8\n",
      "Std_Last_10_Episodes 0.4\n",
      "Best_Avg_10_Episodes -0.8\n",
      "Num_Episodes 26\n",
      "Exploration_Epsilon 0.8836\n",
      "Elapsed_Time_Hours 0.00014527724848853219\n",
      "\n",
      "t = 600 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 700 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  800\n",
      "Avg_Last_10_Episodes -0.8\n",
      "Std_Last_10_Episodes 0.4\n",
      "Best_Avg_10_Episodes -0.8\n",
      "Num_Episodes 33\n",
      "Exploration_Epsilon 0.8448\n",
      "Elapsed_Time_Hours 0.00015666630533006456\n",
      "\n",
      "t = 800 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 900 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  1000\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.8\n",
      "Num_Episodes 41\n",
      "Exploration_Epsilon 0.806\n",
      "Elapsed_Time_Hours 0.00017083313730027942\n",
      "\n",
      "t = 1000 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "\n",
      "t = 1100 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  1200\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.8\n",
      "Num_Episodes 48\n",
      "Exploration_Epsilon 0.7672\n",
      "Elapsed_Time_Hours 0.2046443337202072\n",
      "\n",
      "t = 1200 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "\n",
      "t = 1300 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  1400\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.8\n",
      "Num_Episodes 55\n",
      "Exploration_Epsilon 0.7283999999999999\n",
      "Elapsed_Time_Hours 0.40960374110274844\n",
      "\n",
      "t = 1400 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 1500 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  1600\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.3\n",
      "Best_Avg_10_Episodes -0.8\n",
      "Num_Episodes 63\n",
      "Exploration_Epsilon 0.6896\n",
      "Elapsed_Time_Hours 0.6175528752141529\n",
      "\n",
      "t = 1600 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 1700 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  1800\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.8\n",
      "Num_Episodes 69\n",
      "Exploration_Epsilon 0.6508\n",
      "Elapsed_Time_Hours 0.8320727645688587\n",
      "\n",
      "t = 1800 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 1900 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  2000\n",
      "Avg_Last_10_Episodes -0.7\n",
      "Std_Last_10_Episodes 0.45825756949558405\n",
      "Best_Avg_10_Episodes -0.7\n",
      "Num_Episodes 75\n",
      "Exploration_Epsilon 0.612\n",
      "Elapsed_Time_Hours 1.0389424626694785\n",
      "\n",
      "t = 2000 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 2100 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "Steps:  2200\n",
      "Avg_Last_10_Episodes -0.6\n",
      "Std_Last_10_Episodes 0.4898979485566356\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 81\n",
      "Exploration_Epsilon 0.5731999999999999\n",
      "Elapsed_Time_Hours 1.2449442025025685\n",
      "\n",
      "t = 2200 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 2300 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  2400\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 89\n",
      "Exploration_Epsilon 0.5344\n",
      "Elapsed_Time_Hours 1.4507128023438984\n",
      "\n",
      "t = 2400 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "\n",
      "t = 2500 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "Steps:  2600\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.3\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 93\n",
      "Exploration_Epsilon 0.49560000000000004\n",
      "Elapsed_Time_Hours 1.6618013457457224\n",
      "\n",
      "t = 2600 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 2700 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  2800\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 103\n",
      "Exploration_Epsilon 0.4568\n",
      "Elapsed_Time_Hours 1.8676562564902834\n",
      "\n",
      "t = 2800 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 2900 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  3000\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 109\n",
      "Exploration_Epsilon 0.41800000000000004\n",
      "Elapsed_Time_Hours 2.072900654408667\n",
      "\n",
      "t = 3000 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 3100 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  3200\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 118\n",
      "Exploration_Epsilon 0.3792\n",
      "Elapsed_Time_Hours 2.279055011206203\n",
      "\n",
      "t = 3200 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 3300 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  3400\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 126\n",
      "Exploration_Epsilon 0.3403999999999999\n",
      "Elapsed_Time_Hours 2.486510381566154\n",
      "\n",
      "t = 3400 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 3500 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "Steps:  3600\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 132\n",
      "Exploration_Epsilon 0.3016000000000001\n",
      "Elapsed_Time_Hours 2.6929312250349255\n",
      "\n",
      "t = 3600 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 3700 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  3800\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 143\n",
      "Exploration_Epsilon 0.26280000000000003\n",
      "Elapsed_Time_Hours 2.8986375513341693\n",
      "\n",
      "t = 3800 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 3900 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  4000\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 151\n",
      "Exploration_Epsilon 0.22399999999999998\n",
      "Elapsed_Time_Hours 3.104682284924719\n",
      "\n",
      "t = 4000 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 4100 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  4200\n",
      "Avg_Last_10_Episodes -0.8\n",
      "Std_Last_10_Episodes 0.4\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 157\n",
      "Exploration_Epsilon 0.18520000000000003\n",
      "Elapsed_Time_Hours 3.3107391507095763\n",
      "\n",
      "t = 4200 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 4300 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  4400\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 166\n",
      "Exploration_Epsilon 0.14639999999999997\n",
      "Elapsed_Time_Hours 3.5195394421286053\n",
      "\n",
      "t = 4400 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 4500 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  4600\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 178\n",
      "Exploration_Epsilon 0.10760000000000003\n",
      "Elapsed_Time_Hours 3.726053212285042\n",
      "\n",
      "t = 4600 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 4700 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  4800\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 187\n",
      "Exploration_Epsilon 0.06880000000000008\n",
      "Elapsed_Time_Hours 3.9325475369559393\n",
      "\n",
      "t = 4800 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 4900 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  5000\n",
      "Avg_Last_10_Episodes -0.7\n",
      "Std_Last_10_Episodes 0.6403124237432849\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 197\n",
      "Exploration_Epsilon 0.03\n",
      "Elapsed_Time_Hours 4.139138918651475\n",
      "\n",
      "t = 5000 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 5100 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  5200\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 212\n",
      "Exploration_Epsilon 0.0292\n",
      "Elapsed_Time_Hours 4.348387959665723\n",
      "\n",
      "t = 5200 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 5300 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  5400\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 224\n",
      "Exploration_Epsilon 0.028399999999999998\n",
      "Elapsed_Time_Hours 4.5548200946384005\n",
      "\n",
      "t = 5400 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 5500 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  5600\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 238\n",
      "Exploration_Epsilon 0.0276\n",
      "Elapsed_Time_Hours 4.761802140275638\n",
      "\n",
      "t = 5600 out of max_steps = 50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 5700 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  5800\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 250\n",
      "Exploration_Epsilon 0.026799999999999997\n",
      "Elapsed_Time_Hours 4.96795111225711\n",
      "\n",
      "t = 5800 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 5900 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  6000\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 260\n",
      "Exploration_Epsilon 0.026\n",
      "Elapsed_Time_Hours 5.175316753983497\n",
      "\n",
      "t = 6000 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 6100 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  6200\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 271\n",
      "Exploration_Epsilon 0.0252\n",
      "Elapsed_Time_Hours 5.383684743245443\n",
      "\n",
      "t = 6200 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 6300 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  6400\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 281\n",
      "Exploration_Epsilon 0.024399999999999998\n",
      "Elapsed_Time_Hours 5.589852430025736\n",
      "\n",
      "t = 6400 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 6500 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  6600\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 288\n",
      "Exploration_Epsilon 0.0236\n",
      "Elapsed_Time_Hours 5.79597441414992\n",
      "\n",
      "t = 6600 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 6700 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  6800\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 300\n",
      "Exploration_Epsilon 0.0228\n",
      "Elapsed_Time_Hours 6.002634643250041\n",
      "\n",
      "t = 6800 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 6900 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  7000\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.3\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 312\n",
      "Exploration_Epsilon 0.022\n",
      "Elapsed_Time_Hours 6.210961795051893\n",
      "\n",
      "t = 7000 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 7100 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  7200\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 323\n",
      "Exploration_Epsilon 0.0212\n",
      "Elapsed_Time_Hours 6.4179560614294475\n",
      "\n",
      "t = 7200 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 7300 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  7400\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 331\n",
      "Exploration_Epsilon 0.0204\n",
      "Elapsed_Time_Hours 6.624544643494818\n",
      "\n",
      "t = 7400 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 7500 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  7600\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 340\n",
      "Exploration_Epsilon 0.0196\n",
      "Elapsed_Time_Hours 6.8314610134230715\n",
      "\n",
      "t = 7600 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 7700 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  7800\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 353\n",
      "Exploration_Epsilon 0.018799999999999997\n",
      "Elapsed_Time_Hours 7.040811005234718\n",
      "\n",
      "t = 7800 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 7900 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  8000\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 362\n",
      "Exploration_Epsilon 0.018000000000000002\n",
      "Elapsed_Time_Hours 7.247411177621947\n",
      "\n",
      "t = 8000 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 8100 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  8200\n",
      "Avg_Last_10_Episodes -0.8\n",
      "Std_Last_10_Episodes 0.4\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 375\n",
      "Exploration_Epsilon 0.0172\n",
      "Elapsed_Time_Hours 7.454200324482388\n",
      "\n",
      "t = 8200 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 8300 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  8400\n",
      "Avg_Last_10_Episodes -0.8\n",
      "Std_Last_10_Episodes 0.4\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 386\n",
      "Exploration_Epsilon 0.016399999999999998\n",
      "Elapsed_Time_Hours 7.6606620004442005\n",
      "\n",
      "t = 8400 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 8500 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  8600\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 398\n",
      "Exploration_Epsilon 0.015600000000000001\n",
      "Elapsed_Time_Hours 7.8670616489648815\n",
      "\n",
      "t = 8600 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 8700 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  8800\n",
      "Avg_Last_10_Episodes -0.7\n",
      "Std_Last_10_Episodes 0.45825756949558394\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 410\n",
      "Exploration_Epsilon 0.0148\n",
      "Elapsed_Time_Hours 8.081728072630035\n",
      "\n",
      "t = 8800 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 8900 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  9000\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 419\n",
      "Exploration_Epsilon 0.014000000000000002\n",
      "Elapsed_Time_Hours 8.288216575847732\n",
      "\n",
      "t = 9000 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 9100 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  9200\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 428\n",
      "Exploration_Epsilon 0.013200000000000003\n",
      "Elapsed_Time_Hours 8.494946738613976\n",
      "\n",
      "t = 9200 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 9300 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  9400\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.3\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 440\n",
      "Exploration_Epsilon 0.012400000000000001\n",
      "Elapsed_Time_Hours 8.701023390955395\n",
      "\n",
      "t = 9400 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 9500 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  9600\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 449\n",
      "Exploration_Epsilon 0.0116\n",
      "Elapsed_Time_Hours 8.909969627393616\n",
      "\n",
      "t = 9600 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 9700 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  9800\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 461\n",
      "Exploration_Epsilon 0.010800000000000004\n",
      "Elapsed_Time_Hours 9.116280481351746\n",
      "\n",
      "t = 9800 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 9900 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  10000\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.3\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 474\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 9.323301910691791\n",
      "\n",
      "t = 10000 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 10100 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  10200\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 485\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 9.529703120324347\n",
      "\n",
      "t = 10200 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 10300 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  10400\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 494\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 9.744325058990055\n",
      "\n",
      "t = 10400 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 10500 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  10600\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 506\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 9.95079582883252\n",
      "\n",
      "t = 10600 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 10700 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  10800\n",
      "Avg_Last_10_Episodes -0.7\n",
      "Std_Last_10_Episodes 0.45825756949558394\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 514\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 10.1578016873863\n",
      "\n",
      "t = 10800 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 10900 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  11000\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 525\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 10.364816739029354\n",
      "\n",
      "t = 11000 out of max_steps = 50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 11100 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  11200\n",
      "Avg_Last_10_Episodes -0.8\n",
      "Std_Last_10_Episodes 0.4\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 537\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 10.572224179771212\n",
      "\n",
      "t = 11200 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 11300 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  11400\n",
      "Avg_Last_10_Episodes -0.8\n",
      "Std_Last_10_Episodes 0.4\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 546\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 10.780751384827825\n",
      "\n",
      "t = 11400 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 11500 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  11600\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 557\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 10.987571417159504\n",
      "\n",
      "t = 11600 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 11700 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  11800\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 567\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 11.194634988109271\n",
      "\n",
      "t = 11800 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 11900 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  12000\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 579\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 11.40159312016434\n",
      "\n",
      "t = 12000 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 12100 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  12200\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 591\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 11.61430692381329\n",
      "\n",
      "t = 12200 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 12300 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  12400\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 601\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 11.821585923367076\n",
      "\n",
      "t = 12400 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 12500 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  12600\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.3\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 610\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 12.02902791208691\n",
      "\n",
      "t = 12600 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 12700 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  12800\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 618\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 12.236271645956569\n",
      "\n",
      "t = 12800 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 12900 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  13000\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 629\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 12.452004071540303\n",
      "\n",
      "t = 13000 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 13100 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  13200\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 642\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 12.658832363420062\n",
      "\n",
      "t = 13200 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 13300 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  13400\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 656\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 12.866142686804135\n",
      "\n",
      "t = 13400 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 13500 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  13600\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 667\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 13.072796718279521\n",
      "\n",
      "t = 13600 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 13700 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  13800\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 676\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 13.279835950202411\n",
      "\n",
      "t = 13800 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 13900 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  14000\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 689\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 13.48915512488948\n",
      "\n",
      "t = 14000 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 14100 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  14200\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 697\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 13.695684301786953\n",
      "\n",
      "t = 14200 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 14300 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  14400\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 710\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 13.903010814852184\n",
      "\n",
      "t = 14400 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 14500 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  14600\n",
      "Avg_Last_10_Episodes -0.7\n",
      "Std_Last_10_Episodes 0.6403124237432849\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 719\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 14.110728943347931\n",
      "\n",
      "t = 14600 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 14700 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  14800\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 733\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 14.326102148890495\n",
      "\n",
      "t = 14800 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 14900 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  15000\n",
      "Avg_Last_10_Episodes -0.8\n",
      "Std_Last_10_Episodes 0.4\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 743\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 14.532830081383388\n",
      "\n",
      "t = 15000 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 15100 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  15200\n",
      "Avg_Last_10_Episodes -0.8\n",
      "Std_Last_10_Episodes 0.4\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 754\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 14.740102368262079\n",
      "\n",
      "t = 15200 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 15300 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  15400\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 767\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 14.947255841493607\n",
      "\n",
      "t = 15400 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 15500 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  15600\n",
      "Avg_Last_10_Episodes -0.8\n",
      "Std_Last_10_Episodes 0.4\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 776\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 15.156541280017958\n",
      "\n",
      "t = 15600 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 15700 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  15800\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 784\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 15.362884374260902\n",
      "\n",
      "t = 15800 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 15900 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  16000\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 794\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 15.570386145644719\n",
      "\n",
      "t = 16000 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 16100 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  16200\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 808\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 15.777473150160578\n",
      "\n",
      "t = 16200 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 16300 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  16400\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 816\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 15.987815557519594\n",
      "\n",
      "t = 16400 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 16500 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "Steps:  16600\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 828\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 16.203294676674737\n",
      "\n",
      "t = 16600 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 16700 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  16800\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 839\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 16.411131149397956\n",
      "\n",
      "t = 16800 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 16900 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  17000\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 848\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 16.61955926458041\n",
      "\n",
      "t = 17000 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 17100 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  17200\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.3\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 856\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 16.827858971622256\n",
      "\n",
      "t = 17200 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 17300 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  17400\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 869\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 17.038395905097325\n",
      "\n",
      "t = 17400 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 17500 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  17600\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 880\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 17.24629606001907\n",
      "\n",
      "t = 17600 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 17700 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  17800\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 889\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 17.45259200539854\n",
      "\n",
      "t = 17800 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 17900 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  18000\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 898\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 17.65896003286044\n",
      "\n",
      "t = 18000 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 18100 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  18200\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 908\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 17.873757820791667\n",
      "\n",
      "t = 18200 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 18300 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  18400\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 920\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 18.081949358847407\n",
      "\n",
      "t = 18400 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 18500 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  18600\n",
      "Avg_Last_10_Episodes -0.8\n",
      "Std_Last_10_Episodes 0.4\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 931\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 18.290223680271044\n",
      "\n",
      "t = 18600 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 18700 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  18800\n",
      "Avg_Last_10_Episodes -0.8\n",
      "Std_Last_10_Episodes 0.6000000000000001\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 945\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 18.49671886152691\n",
      "\n",
      "t = 18800 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 18900 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  19000\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 956\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 18.703287190000218\n",
      "\n",
      "t = 19000 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 19100 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  19200\n",
      "Avg_Last_10_Episodes -0.8\n",
      "Std_Last_10_Episodes 0.4\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 967\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 18.91324093679587\n",
      "\n",
      "t = 19200 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 19300 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  19400\n",
      "Avg_Last_10_Episodes -1.0\n",
      "Std_Last_10_Episodes 0.0\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 979\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 19.121352295941776\n",
      "\n",
      "t = 19400 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 19500 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  19600\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 990\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 19.330450512369474\n",
      "\n",
      "t = 19600 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 19700 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  19800\n",
      "Avg_Last_10_Episodes -0.9\n",
      "Std_Last_10_Episodes 0.30000000000000004\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 998\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 19.54025618195534\n",
      "\n",
      "t = 19800 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "\n",
      "t = 19900 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "Steps:  20000\n",
      "Avg_Last_10_Episodes -0.8\n",
      "Std_Last_10_Episodes 0.4\n",
      "Best_Avg_10_Episodes -0.6\n",
      "Num_Episodes 1004\n",
      "Exploration_Epsilon 0.01\n",
      "Elapsed_Time_Hours 19.756292535795104\n",
      "\n",
      "t = 20000 out of max_steps = 50000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17600/3411839467.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mdouble_q\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdouble_q\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mmax_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mcartpole\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     )\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17600/2419633173.py\u001b[0m in \u001b[0;36mdqn_learn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0malg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m#print(\"update model\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0malg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;31m#print(\"log progress\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0malg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17600/2258947070.py\u001b[0m in \u001b[0;36mupdate_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mreward_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[0mnext_state_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m             \u001b[0mstate_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mreward_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_actions = env.action_space.n\n",
    "\n",
    "#input dim 150x150x3 --maxpool--> 15x15x3\n",
    "policy_net = DQN(15*15*3, n_actions).to(device)\n",
    "target_net = DQN(15*15*3, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=5e-4)\n",
    "\n",
    "steps, mean_rewards = dqn_learn(\n",
    "    env=env,\n",
    "    q_func=policy_net,\n",
    "    t_func=target_net,\n",
    "    optimizer_spec=optimizer,\n",
    "    exploration=exploration_schedule,\n",
    "    replay_buffer_size=50000,\n",
    "    batch_size=100,\n",
    "    gamma=0.99,\n",
    "    learning_starts=1000,\n",
    "    target_update_freq=100,\n",
    "    grad_norm_clipping=10,\n",
    "    double_q=double_q,\n",
    "    max_steps=num_steps,\n",
    "    cartpole=True\n",
    "    )\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mpfh0lFX2wiu",
    "outputId": "fa7280f7-c0fd-4409-8cb6-925ba2f5b100"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(steps, mean_rewards)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Average Return\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CS5446_DQN_trial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
